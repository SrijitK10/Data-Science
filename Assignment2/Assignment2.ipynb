{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading the athlete performance dataset...\")\n",
    "    df = pd.read_csv(\"data/athlete_performance_raw.csv\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Handling Missing Data\n",
    "def handle_missing_data(df):\n",
    "    print(\"\\n--- Task 1: Handling Missing Data ---\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df) * 100).round(2)\n",
    "    \n",
    "    # Display columns with missing values\n",
    "    print(\"Columns with missing values:\")\n",
    "    for col in df.columns:\n",
    "        if missing_values[col] > 0:\n",
    "            print(f\"  {col}: {missing_values[col]} missing values ({missing_percent[col]}%)\")\n",
    "    \n",
    "    # Handling strategy for each column with missing values\n",
    "    \n",
    "    # 1. Heart_Rate: Impute with median based on similar training sessions (by distance range)\n",
    "    if missing_values['Heart_Rate'] > 0:\n",
    "        print(\"\\nImputing missing Heart_Rate values based on similar training distances...\")\n",
    "        \n",
    "        # Create distance bins\n",
    "        df['Distance_Bin'] = pd.cut(df['Distance'], bins=5)\n",
    "        \n",
    "        # Fill missing heart rates with the median of the same distance bin\n",
    "        df['Heart_Rate'] = df.groupby('Distance_Bin')['Heart_Rate'].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "        \n",
    "        # Remove the temporary bin column\n",
    "        df = df.drop(columns=['Distance_Bin'])\n",
    "    \n",
    "    # 2. Calorie_Intake: Impute with median value for the same food item\n",
    "    if missing_values['Calorie_Intake'] > 0:\n",
    "        print(\"Imputing missing Calorie_Intake values based on the food item...\")\n",
    "        \n",
    "        # Group by food item to get the median calorie intake\n",
    "        food_median_calories = df.groupby('Food_Item')['Calorie_Intake'].median()\n",
    "        \n",
    "        # For each food item with missing calories, fill with the median for that food\n",
    "        for food in food_median_calories.index:\n",
    "            mask = (df['Food_Item'] == food) & (df['Calorie_Intake'].isna())\n",
    "            df.loc[mask, 'Calorie_Intake'] = food_median_calories[food]\n",
    "    \n",
    "    # 3. Hydration_Level: Impute with median based on training effort and distance\n",
    "    if missing_values['Hydration_Level'] > 0:\n",
    "        print(\"Imputing missing Hydration_Level values based on training effort and distance...\")\n",
    "        \n",
    "        # First, make sure there are no negative hydration levels affecting the median\n",
    "        hydration_for_median = df[df['Hydration_Level'] > 0]['Hydration_Level']\n",
    "        median_hydration = hydration_for_median.median()\n",
    "        \n",
    "        # Fill missing values with overall median\n",
    "        df['Hydration_Level'] = df['Hydration_Level'].fillna(median_hydration)\n",
    "    \n",
    "    # Check if any missing values remain\n",
    "    missing_after = df.isnull().sum()\n",
    "    if missing_after.sum() > 0:\n",
    "        print(\"\\nRemaining missing values after imputation:\")\n",
    "        for col in df.columns:\n",
    "            if missing_after[col] > 0:\n",
    "                print(f\"  {col}: {missing_after[col]} missing values\")\n",
    "    else:\n",
    "        print(\"\\nAll missing values have been handled successfully.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Removing Duplicate Records\n",
    "def remove_duplicates(df):\n",
    "    print(\"\\n--- Task 2: Removing Duplicate Records ---\")\n",
    "    \n",
    "    # Count initial records\n",
    "    initial_count = len(df)\n",
    "    print(f\"Initial record count: {initial_count}\")\n",
    "    \n",
    "    # Check for exact duplicates first\n",
    "    exact_duplicates = df.duplicated().sum()\n",
    "    print(f\"Exact duplicate records: {exact_duplicates}\")\n",
    "    \n",
    "    # Define which columns should be considered for identifying training session duplicates\n",
    "    session_key_columns = ['Session_ID', 'Date', 'Time']\n",
    "    \n",
    "    # Find duplicates based on session identifiers\n",
    "    session_duplicates = df.duplicated(subset=session_key_columns, keep=False)\n",
    "    \n",
    "    if session_duplicates.sum() > 0:\n",
    "        print(f\"\\nFound {session_duplicates.sum()} records with duplicate session identifiers\")\n",
    "        \n",
    "        # Display some examples of duplicates\n",
    "        print(\"\\nSample session duplicates:\")\n",
    "        duplicate_examples = df[session_duplicates].sort_values(by=session_key_columns).head(10)\n",
    "        print(duplicate_examples[session_key_columns + ['Distance', 'Speed', 'Heart_Rate']].to_string())\n",
    "        \n",
    "        # Keep the record with fewer missing values when there are duplicates\n",
    "        df['missing_count'] = df.isnull().sum(axis=1)\n",
    "        \n",
    "        # Sort by session keys and missing count, then remove duplicates\n",
    "        df = df.sort_values(by=session_key_columns + ['missing_count'])\n",
    "        df = df.drop_duplicates(subset=session_key_columns, keep='first')\n",
    "        df = df.drop(columns=['missing_count'])\n",
    "        \n",
    "        # Count records after removing duplicates\n",
    "        final_count = len(df)\n",
    "        print(f\"\\nRemoved {initial_count - final_count} duplicate records\")\n",
    "        print(f\"Record count after removing duplicates: {final_count}\")\n",
    "    else:\n",
    "        print(\"No session duplicates found.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Standardizing Data Formats\n",
    "def standardize_formats(df):\n",
    "    print(\"\\n--- Task 3: Standardizing Data Formats ---\")\n",
    "    \n",
    "    # 1. Standardize Time format to 24-hour\n",
    "    print(\"Standardizing time format to 24-hour...\")\n",
    "    \n",
    "    # Check current time formats\n",
    "    time_patterns = df['Time'].str.contains('AM|PM', case=False).sum()\n",
    "    print(f\"Records with 12-hour format (AM/PM): {time_patterns}\")\n",
    "    print(f\"Records with 24-hour format: {len(df) - time_patterns}\")\n",
    "    \n",
    "    # Function to convert 12-hour time to 24-hour format\n",
    "    def convert_to_24hr(time_str):\n",
    "        if isinstance(time_str, str) and ('AM' in time_str.upper() or 'PM' in time_str.upper()):\n",
    "            # Parse 12-hour format\n",
    "            try:\n",
    "                if 'AM' in time_str.upper():\n",
    "                    # Remove AM and handle 12 AM special case\n",
    "                    time_str = time_str.upper().replace('AM', '').strip()\n",
    "                    hour, minute = map(int, time_str.split(':'))\n",
    "                    hour = 0 if hour == 12 else hour\n",
    "                    return f\"{hour:02d}:{minute:02d}\"\n",
    "                else:  # PM\n",
    "                    # Remove PM and handle 12 PM special case\n",
    "                    time_str = time_str.upper().replace('PM', '').strip()\n",
    "                    hour, minute = map(int, time_str.split(':'))\n",
    "                    hour = hour if hour == 12 else hour + 12\n",
    "                    return f\"{hour:02d}:{minute:02d}\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting time: {time_str}, Error: {e}\")\n",
    "                return time_str\n",
    "        return time_str  # Already in 24-hour format\n",
    "    \n",
    "    # Apply the conversion function\n",
    "    df['Time'] = df['Time'].apply(convert_to_24hr)\n",
    "    \n",
    "    # 2. Standardize Distance to kilometers\n",
    "    print(\"\\nStandardizing distance to kilometers...\")\n",
    "    \n",
    "    # Check current distance units\n",
    "    miles_count = (df['Distance_Unit'] == 'miles').sum()\n",
    "    km_count = (df['Distance_Unit'] == 'km').sum()\n",
    "    print(f\"Records with distance in miles: {miles_count}\")\n",
    "    print(f\"Records with distance in kilometers: {km_count}\")\n",
    "    \n",
    "    # Convert miles to kilometers where needed\n",
    "    miles_mask = df['Distance_Unit'] == 'miles'\n",
    "    df.loc[miles_mask, 'Distance'] = df.loc[miles_mask, 'Distance'] * 1.60934\n",
    "    df.loc[miles_mask, 'Distance_Unit'] = 'km'\n",
    "    \n",
    "    # Round distance to 2 decimal places for consistency\n",
    "    df['Distance'] = df['Distance'].round(2)\n",
    "    \n",
    "    # Verify standardization\n",
    "    print(f\"\\nAfter standardization, records with distance in km: {(df['Distance_Unit'] == 'km').sum()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Identifying and Handling Outliers\n",
    "def handle_outliers(df):\n",
    "    print(\"\\n--- Task 4: Identifying and Handling Outliers ---\")\n",
    "    \n",
    "    # Focus on detecting outliers in the Speed column\n",
    "    print(\"Detecting outliers in the Speed column...\")\n",
    "    \n",
    "    # Plot speed distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x=df['Speed'])\n",
    "    plt.title('Speed Distribution (Before Outlier Handling)')\n",
    "    plt.savefig('data/speed_distribution_before.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate IQR for Speed\n",
    "    Q1 = df['Speed'].quantile(0.25)\n",
    "    Q3 = df['Speed'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    speed_outliers = df[(df['Speed'] < lower_bound) | (df['Speed'] > upper_bound)]\n",
    "    outlier_count = len(speed_outliers)\n",
    "    \n",
    "    print(f\"Speed quartiles - Q1: {Q1:.2f}, Median: {df['Speed'].median():.2f}, Q3: {Q3:.2f}\")\n",
    "    print(f\"Speed IQR: {IQR:.2f}\")\n",
    "    print(f\"Outlier bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"Detected {outlier_count} outliers in Speed column\")\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        # Display outliers\n",
    "        print(\"\\nSample speed outliers:\")\n",
    "        print(speed_outliers[['Session_ID', 'Date', 'Speed', 'Distance', 'Heart_Rate']].head().to_string())\n",
    "        \n",
    "        # Method: Cap outliers at the bounds\n",
    "        print(\"\\nCapping extreme speed values at the calculated bounds...\")\n",
    "        df['Speed'] = df['Speed'].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        # Plot speed distribution after handling outliers\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=df['Speed'])\n",
    "        plt.title('Speed Distribution (After Outlier Handling)')\n",
    "        plt.savefig('data/speed_distribution_after.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Verify outliers were handled\n",
    "        remaining_outliers = df[(df['Speed'] < lower_bound) | (df['Speed'] > upper_bound)]\n",
    "        print(f\"Remaining outliers after capping: {len(remaining_outliers)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Correcting Data Entry Errors\n",
    "def correct_data_errors(df):\n",
    "    print(\"\\n--- Task 5: Correcting Data Entry Errors ---\")\n",
    "    \n",
    "    # 1. Correct incorrect calorie values for food items\n",
    "    print(\"Correcting invalid calorie values for food items...\")\n",
    "    \n",
    "    # Define correct calorie values for common food items\n",
    "    correct_calories = {\n",
    "        \"Banana\": 105,\n",
    "        \"Energy Bar\": 250,\n",
    "        \"Oatmeal\": 150,\n",
    "        \"Protein Shake\": 300,\n",
    "        \"Yogurt\": 120,\n",
    "        \"Pasta\": 400,\n",
    "        \"Chicken Breast\": 165,\n",
    "        \"Salmon\": 230,\n",
    "        \"Rice\": 210,\n",
    "        \"Sweet Potato\": 115\n",
    "    }\n",
    "    \n",
    "    # Identify anomalous calorie values (much higher than expected)\n",
    "    anomalies = 0\n",
    "    for food, correct_value in correct_calories.items():\n",
    "        # Consider values significantly higher than correct as errors\n",
    "        mask = (df['Food_Item'] == food) & (df['Calorie_Intake'] > correct_value * 5)\n",
    "        anomaly_count = mask.sum()\n",
    "        \n",
    "        if anomaly_count > 0:\n",
    "            anomalies += anomaly_count\n",
    "            print(f\"  Found {anomaly_count} incorrect calorie values for {food}\")\n",
    "            df.loc[mask, 'Calorie_Intake'] = correct_value\n",
    "    \n",
    "    print(f\"Total incorrect calorie values corrected: {anomalies}\")\n",
    "    \n",
    "    # 2. Fix negative hydration levels\n",
    "    negative_hydration = (df['Hydration_Level'] < 0).sum()\n",
    "    if negative_hydration > 0:\n",
    "        print(f\"\\nFound {negative_hydration} records with negative hydration levels\")\n",
    "        \n",
    "        # Fix by taking the absolute value\n",
    "        df.loc[df['Hydration_Level'] < 0, 'Hydration_Level'] = df.loc[df['Hydration_Level'] < 0, 'Hydration_Level'].abs()\n",
    "        \n",
    "        print(\"Corrected negative hydration levels by converting to positive values\")\n",
    "    else:\n",
    "        print(\"\\nNo negative hydration levels found\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Dropping Irrelevant Columns\n",
    "def drop_irrelevant_columns(df):\n",
    "    print(\"\\n--- Task 6: Dropping Irrelevant Columns ---\")\n",
    "    \n",
    "    # Identify columns that don't contribute to performance analysis\n",
    "    irrelevant_columns = [\n",
    "        'Athlete_Social_Media_Followers',\n",
    "        'Weather_Forecast_Accuracy',\n",
    "        'Local_Race_Participants',\n",
    "        'Training_Plan_Version'\n",
    "    ]\n",
    "    \n",
    "    # Check if these columns exist in the dataset\n",
    "    existing_irrelevant = [col for col in irrelevant_columns if col in df.columns]\n",
    "    \n",
    "    if existing_irrelevant:\n",
    "        print(f\"Dropping {len(existing_irrelevant)} irrelevant columns:\")\n",
    "        for col in existing_irrelevant:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        # Drop the irrelevant columns\n",
    "        df = df.drop(columns=existing_irrelevant)\n",
    "        \n",
    "        print(f\"\\nRemaining columns after dropping irrelevant ones: {df.shape[1]}\")\n",
    "    else:\n",
    "        print(\"No irrelevant columns found\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Final Dataset Preparation\n",
    "def prepare_final_dataset(df):\n",
    "    print(\"\\n--- Task 7: Final Dataset Preparation ---\")\n",
    "    \n",
    "    # 1. Ensure proper data types\n",
    "    print(\"Ensuring proper data types...\")\n",
    "    \n",
    "    # Convert numeric columns to appropriate types\n",
    "    numeric_columns = ['Distance', 'Speed', 'Heart_Rate', 'Calories_Burned', \n",
    "                       'Temperature', 'Humidity', 'Calorie_Intake', \n",
    "                       'Hydration_Level', 'Training_Effort', 'Session_Rating',\n",
    "                       'Post_Recovery_Score']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            if col in ['Heart_Rate', 'Calories_Burned', 'Calorie_Intake', \n",
    "                       'Training_Effort', 'Session_Rating', 'Post_Recovery_Score']:\n",
    "                df[col] = df[col].astype(int)\n",
    "            else:\n",
    "                df[col] = df[col].astype(float)\n",
    "    \n",
    "    # 2. Create derived metrics for analysis\n",
    "    print(\"\\nCreating derived metrics for analysis...\")\n",
    "    \n",
    "    # Calculate calories burned per kilometer\n",
    "    df['Calories_Per_Km'] = (df['Calories_Burned'] / df['Distance']).round(1)\n",
    "    \n",
    "    # Calculate training efficiency (speed relative to heart rate)\n",
    "    df['Training_Efficiency'] = (df['Speed'] / df['Heart_Rate'] * 100).round(2)\n",
    "    \n",
    "    # Calculate hydration ratio (hydration level per kilometer)\n",
    "    df['Hydration_Ratio'] = (df['Hydration_Level'] / df['Distance']).round(2)\n",
    "    \n",
    "    # 3. Sort dataset by date and time for better analysis\n",
    "    df = df.sort_values(by=['Date', 'Time'])\n",
    "    \n",
    "    # 4. Generate a final dataset summary\n",
    "    print(\"\\nFinal dataset info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nFinal dataset descriptive statistics:\")\n",
    "    print(df.describe().round(2))\n",
    "    \n",
    "    # 5. Save the cleaned dataset\n",
    "    df.to_csv('data/athlete_performance_cleaned.csv', index=False)\n",
    "    print(\"\\nFinal cleaned dataset saved to: data/athlete_performance_cleaned.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the athlete performance dataset...\n",
      "Original dataset shape: (110, 22)\n",
      "\n",
      "--- Task 1: Handling Missing Data ---\n",
      "Columns with missing values:\n",
      "  Heart_Rate: 15 missing values (13.64%)\n",
      "  Calorie_Intake: 10 missing values (9.09%)\n",
      "  Hydration_Level: 13 missing values (11.82%)\n",
      "  Notes: 17 missing values (15.45%)\n",
      "\n",
      "Imputing missing Heart_Rate values based on similar training distances...\n",
      "Imputing missing Calorie_Intake values based on the food item...\n",
      "Imputing missing Hydration_Level values based on training effort and distance...\n",
      "\n",
      "Remaining missing values after imputation:\n",
      "  Notes: 17 missing values\n",
      "\n",
      "--- Task 2: Removing Duplicate Records ---\n",
      "Initial record count: 110\n",
      "Exact duplicate records: 0\n",
      "\n",
      "Found 20 records with duplicate session identifiers\n",
      "\n",
      "Sample session duplicates:\n",
      "     Session_ID        Date     Time  Distance  Speed  Heart_Rate\n",
      "17           18  2023-01-18    19:15     11.92   3.07       141.0\n",
      "105          18  2023-01-18    19:15     11.92   3.07       141.0\n",
      "19           20  2023-01-20    07:45     16.73   3.94       152.0\n",
      "107          20  2023-01-20    07:45     16.73   3.94       152.0\n",
      "28           29  2023-01-29    09:00      5.49   4.42       180.0\n",
      "104          29  2023-01-29    09:00      5.49   4.42       180.0\n",
      "37           38  2023-01-08    10:30      6.15   3.70       141.0\n",
      "101          38  2023-01-08    10:30      6.15   3.70       141.0\n",
      "47           48  2023-01-18  7:45 AM     15.91   3.47       142.0\n",
      "102          48  2023-01-18  7:45 AM     15.91   3.47       142.0\n",
      "\n",
      "Removed 10 duplicate records\n",
      "Record count after removing duplicates: 100\n",
      "\n",
      "--- Task 3: Standardizing Data Formats ---\n",
      "Standardizing time format to 24-hour...\n",
      "Records with 12-hour format (AM/PM): 55\n",
      "Records with 24-hour format: 45\n",
      "\n",
      "Standardizing distance to kilometers...\n",
      "Records with distance in miles: 49\n",
      "Records with distance in kilometers: 51\n",
      "\n",
      "After standardization, records with distance in km: 100\n",
      "\n",
      "--- Task 4: Identifying and Handling Outliers ---\n",
      "Detecting outliers in the Speed column...\n",
      "Speed quartiles - Q1: 3.73, Median: 4.36, Q3: 5.17\n",
      "Speed IQR: 1.45\n",
      "Outlier bounds: [1.56, 7.34]\n",
      "Detected 2 outliers in Speed column\n",
      "\n",
      "Sample speed outliers:\n",
      "    Session_ID        Date  Speed  Distance  Heart_Rate\n",
      "34          35  2023-01-05   1.03      8.66       155.0\n",
      "80          81  2023-01-21   9.80     10.83       143.0\n",
      "\n",
      "Capping extreme speed values at the calculated bounds...\n",
      "Remaining outliers after capping: 0\n",
      "\n",
      "--- Task 5: Correcting Data Entry Errors ---\n",
      "Correcting invalid calorie values for food items...\n",
      "  Found 3 incorrect calorie values for Banana\n",
      "  Found 1 incorrect calorie values for Oatmeal\n",
      "  Found 2 incorrect calorie values for Protein Shake\n",
      "  Found 2 incorrect calorie values for Yogurt\n",
      "  Found 1 incorrect calorie values for Pasta\n",
      "  Found 1 incorrect calorie values for Chicken Breast\n",
      "  Found 1 incorrect calorie values for Salmon\n",
      "  Found 3 incorrect calorie values for Rice\n",
      "  Found 2 incorrect calorie values for Sweet Potato\n",
      "Total incorrect calorie values corrected: 16\n",
      "\n",
      "Found 1 records with negative hydration levels\n",
      "Corrected negative hydration levels by converting to positive values\n",
      "\n",
      "--- Task 6: Dropping Irrelevant Columns ---\n",
      "Dropping 4 irrelevant columns:\n",
      "  - Athlete_Social_Media_Followers\n",
      "  - Weather_Forecast_Accuracy\n",
      "  - Local_Race_Participants\n",
      "  - Training_Plan_Version\n",
      "\n",
      "Remaining columns after dropping irrelevant ones: 18\n",
      "\n",
      "--- Task 7: Final Dataset Preparation ---\n",
      "Ensuring proper data types...\n",
      "\n",
      "Creating derived metrics for analysis...\n",
      "\n",
      "Final dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100 entries, 0 to 29\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Session_ID           100 non-null    int64  \n",
      " 1   Date                 100 non-null    object \n",
      " 2   Time                 100 non-null    object \n",
      " 3   Distance             100 non-null    float64\n",
      " 4   Distance_Unit        100 non-null    object \n",
      " 5   Speed                100 non-null    float64\n",
      " 6   Heart_Rate           100 non-null    int64  \n",
      " 7   Calories_Burned      100 non-null    int64  \n",
      " 8   Temperature          100 non-null    float64\n",
      " 9   Humidity             100 non-null    float64\n",
      " 10  Food_Item            100 non-null    object \n",
      " 11  Calorie_Intake       100 non-null    int64  \n",
      " 12  Hydration_Level      100 non-null    float64\n",
      " 13  Training_Effort      100 non-null    int64  \n",
      " 14  Session_Rating       100 non-null    int64  \n",
      " 15  Shoe_Brand           100 non-null    object \n",
      " 16  Post_Recovery_Score  100 non-null    int64  \n",
      " 17  Notes                85 non-null     object \n",
      " 18  Calories_Per_Km      100 non-null    float64\n",
      " 19  Training_Efficiency  100 non-null    float64\n",
      " 20  Hydration_Ratio      100 non-null    float64\n",
      "dtypes: float64(8), int64(7), object(6)\n",
      "memory usage: 17.2+ KB\n",
      "None\n",
      "\n",
      "Final dataset descriptive statistics:\n",
      "       Session_ID  Distance   Speed  Heart_Rate  Calories_Burned  Temperature  \\\n",
      "count      100.00    100.00  100.00      100.00           100.00       100.00   \n",
      "mean        50.50     12.58    4.41      148.12           761.26        22.37   \n",
      "std         29.01      4.44    0.91       16.47           273.37         7.22   \n",
      "min          1.00      5.15    1.56      120.00           295.00        10.20   \n",
      "25%         25.75      8.65    3.73      135.75           518.25        17.20   \n",
      "50%         50.50     12.18    4.36      144.00           741.50        21.75   \n",
      "75%         75.25     16.76    5.17      162.25          1009.75        28.18   \n",
      "max        100.00     20.74    7.34      180.00          1238.00        34.70   \n",
      "\n",
      "       Humidity  Calorie_Intake  Hydration_Level  Training_Effort  \\\n",
      "count    100.00          100.00           100.00           100.00   \n",
      "mean      60.47          204.25             1.74             5.86   \n",
      "std       18.56           85.12             0.71             3.15   \n",
      "min       30.00          105.00             0.50             1.00   \n",
      "25%       42.75          120.00             1.18             3.00   \n",
      "50%       61.00          210.00             1.80             6.00   \n",
      "75%       77.00          250.00             2.30             9.00   \n",
      "max       90.00          400.00             3.00            10.00   \n",
      "\n",
      "       Session_Rating  Post_Recovery_Score  Calories_Per_Km  \\\n",
      "count          100.00               100.00           100.00   \n",
      "mean             3.15                 5.85            60.45   \n",
      "std              1.30                 2.98             3.34   \n",
      "min              1.00                 1.00            54.20   \n",
      "25%              2.00                 3.00            57.65   \n",
      "50%              3.00                 6.00            60.90   \n",
      "75%              4.00                 8.00            62.82   \n",
      "max              5.00                10.00            66.00   \n",
      "\n",
      "       Training_Efficiency  Hydration_Ratio  \n",
      "count               100.00           100.00  \n",
      "mean                  3.01             0.16  \n",
      "std                   0.71             0.11  \n",
      "min                   1.01             0.03  \n",
      "25%                   2.48             0.09  \n",
      "50%                   2.98             0.13  \n",
      "75%                   3.46             0.20  \n",
      "max                   5.13             0.56  \n",
      "\n",
      "Final cleaned dataset saved to: data/athlete_performance_cleaned.csv\n",
      "\n",
      "Data cleaning complete!\n",
      "\n",
      "Summary of Cleaning Actions:\n",
      "1. Handled missing values in Heart_Rate, Calorie_Intake, and Hydration_Level columns\n",
      "2. Removed duplicate training sessions\n",
      "3. Standardized time format to 24-hour and distance to kilometers\n",
      "4. Identified and handled outliers in the Speed column\n",
      "5. Corrected erroneous calorie values and negative hydration levels\n",
      "6. Removed irrelevant columns not related to performance analysis\n",
      "7. Created derived metrics for deeper analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rw/g4p48pvn7w946c84lc3b8sdc0000gn/T/ipykernel_66011/3899851911.py:25: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df['Heart_Rate'] = df.groupby('Distance_Bin')['Heart_Rate'].transform(\n"
     ]
    }
   ],
   "source": [
    "# Main function to run all cleaning tasks\n",
    "def main():\n",
    "    # Load the data\n",
    "    df = load_data()\n",
    "    \n",
    "    # Apply each cleaning task\n",
    "    df = handle_missing_data(df)\n",
    "    df = remove_duplicates(df)\n",
    "    df = standardize_formats(df)\n",
    "    df = handle_outliers(df)\n",
    "    df = correct_data_errors(df)\n",
    "    df = drop_irrelevant_columns(df)\n",
    "    df = prepare_final_dataset(df)\n",
    "    \n",
    "    print(\"\\nData cleaning complete!\")\n",
    "    \n",
    "    # Generate a simple summary of changes made\n",
    "    print(\"\\nSummary of Cleaning Actions:\")\n",
    "    print(\"1. Handled missing values in Heart_Rate, Calorie_Intake, and Hydration_Level columns\")\n",
    "    print(\"2. Removed duplicate training sessions\")\n",
    "    print(\"3. Standardized time format to 24-hour and distance to kilometers\")\n",
    "    print(\"4. Identified and handled outliers in the Speed column\")\n",
    "    print(\"5. Corrected erroneous calorie values and negative hydration levels\")\n",
    "    print(\"6. Removed irrelevant columns not related to performance analysis\")\n",
    "    print(\"7. Created derived metrics for deeper analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
